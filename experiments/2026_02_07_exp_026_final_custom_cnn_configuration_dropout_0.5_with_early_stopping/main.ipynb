{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3934,
     "status": "ok",
     "timestamp": 1770438894895,
     "user": {
      "displayName": "Inzaman Sheshan",
      "userId": "03733916943512366575"
     },
     "user_tz": -330
    },
    "id": "78LMvV9WCkzf",
    "outputId": "f6752cc4-9d26-43c4-fe24-7a8173d6e73b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1770438894914,
     "user": {
      "displayName": "Inzaman Sheshan",
      "userId": "03733916943512366575"
     },
     "user_tz": -330
    },
    "id": "YXYvXqkiCXnw",
    "outputId": "be10352c-648a-4300-c7eb-618734d444ec"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nExperiment: 2026_02_07_exp_026_final_custom_cnn_configuration_dropout_0.5_with_early_stopping\\nGoal:\\nDataset:\\nNotes:\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Experiment: 2026_02_07_exp_026_final_custom_cnn_configuration_dropout_0.5_with_early_stopping\n",
    "Goal:\n",
    "Dataset:\n",
    "Notes:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1770438894967,
     "user": {
      "displayName": "Inzaman Sheshan",
      "userId": "03733916943512366575"
     },
     "user_tz": -330
    },
    "id": "vsy9F0Y7Cgw_",
    "outputId": "090de848-6865-44d7-fa46-55865a2de0ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Colab Notebooks/Data Science Group Project\n",
      "['data', 'experiments', 'JustTests', 'OLD', 'MoveImagesFinal.ipynb', 'Splitting Dataset Into Train_Validation_Test_Sets.ipynb', 'RemovingBlackFinal.ipynb', 'Top-View Image Selection From MRI and CT Dataset.ipynb', 'old experiments']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/content/drive/My Drive/Colab Notebooks/Data Science Group Project\")\n",
    "\n",
    "print(os.getcwd())\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1770438894989,
     "user": {
      "displayName": "Inzaman Sheshan",
      "userId": "03733916943512366575"
     },
     "user_tz": -330
    },
    "id": "6eu6SOCyCOgI"
   },
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Imports\n",
    "# =====================================================\n",
    "\n",
    "import platform\n",
    "from pathlib import Path\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "import yaml\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# =====================================================\n",
    "# Config & Reproducibility\n",
    "# =====================================================\n",
    "\n",
    "def load_config(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Dataset\n",
    "# =====================================================\n",
    "\n",
    "class DualImageDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        # Load once\n",
    "        self.raw_imgs, self.proc_imgs, self.labels = torch.load(path)\n",
    "\n",
    "        # Ensure proper dtype\n",
    "        self.raw_imgs = self.raw_imgs.float()\n",
    "        self.proc_imgs = self.proc_imgs.float()\n",
    "        self.labels = self.labels.long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.raw_imgs[idx], self.proc_imgs[idx], self.labels[idx]\n",
    "\n",
    "def dataset(transformed_data_dir, cfg):\n",
    "    train_dataset = DualImageDataset(path=transformed_data_dir / cfg[\"data\"][\"train_path\"])\n",
    "    val_dataset = DualImageDataset(path=transformed_data_dir / cfg[\"data\"][\"val_path\"])\n",
    "    test_dataset = DualImageDataset(path=transformed_data_dir / cfg[\"data\"][\"test_path\"])\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Dataloaders\n",
    "# =====================================================\n",
    "\n",
    "def dataloader(cfg, train_dataset, val_dataset, test_dataset):\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=cfg[\"training\"][\"batch_size\"], shuffle=True, num_workers=0, pin_memory=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=cfg[\"training\"][\"batch_size\"], shuffle=False, num_workers=0, pin_memory=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=cfg[\"training\"][\"batch_size\"], shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "    print(f\"Number of validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Number of testing samples: {len(test_dataset)}\")\n",
    "\n",
    "    print(f\"Length of TrainDataloader: {len(train_dataloader)} batches of {cfg['training']['batch_size']}\")\n",
    "    print(f\"Length of ValDataloader: {len(val_dataloader)} batches of {cfg['training']['batch_size']}\")\n",
    "    print(f\"Length of TestDataloader: {len(test_dataloader)} batches of {cfg['training']['batch_size']}\")\n",
    "\n",
    "    return train_dataloader, val_dataloader, test_dataloader\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Model\n",
    "# =====================================================\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_units, output_shape, dropout, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        # Compute flatten size dynamically\n",
    "        with torch.no_grad():\n",
    "            x = torch.zeros(1, input_shape, *tuple(cfg[\"data\"][\"image_size\"]))  # batch_size=1, input_shape channels\n",
    "            x = self.conv_block_1(x)\n",
    "            x = self.conv_block_2(x)\n",
    "            n_features = x.numel() // x.shape[0]  # total features per sample\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=n_features, out_features=output_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, raw_img, processed_img):\n",
    "        x = torch.cat([raw_img, processed_img], dim=1)  # Concatenate raw + processed channels -> [B, 2, H, W]\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_block_2(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def make_model(cfg, classes, device):\n",
    "    model = CustomCNN(input_shape=cfg[\"model\"][\"input_dim\"], hidden_units=cfg[\"model\"][\"hidden_units\"],\n",
    "                      output_shape=len(classes), dropout=cfg[\"model\"][\"dropout\"], cfg=cfg).to(device)\n",
    "\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=cfg[\"training\"][\"lr\"])\n",
    "\n",
    "    return model, loss_func, optimizer\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Training / Evaluation Utils\n",
    "# =====================================================\n",
    "\n",
    "# =====================================================\n",
    "# Train\n",
    "# =====================================================\n",
    "\n",
    "def train_step(model, train_dataloader, loss_func, optimizer, device):\n",
    "    train_loss, train_acc = 0, 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch, (raw_X, processed_X, y) in enumerate(train_dataloader):\n",
    "        raw_X, processed_X, y = raw_X.to(device, non_blocking=True), processed_X.to(device, non_blocking=True), y.to(device)\n",
    "\n",
    "        train_y_pred = model(raw_X, processed_X)\n",
    "\n",
    "        loss = loss_func(train_y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # accuracy\n",
    "        correct += (train_y_pred.argmax(dim=1) == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_acc = 100.0 * correct / total\n",
    "\n",
    "    print(f\"Train loss: {train_loss:.5f} | Train acc: {train_acc:.2f}%\\n\")\n",
    "    return train_loss, train_acc\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Validation\n",
    "# =====================================================\n",
    "\n",
    "def val_step(model, val_dataloader, loss_func, device):\n",
    "    val_loss, val_acc = 0, 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for raw_X, processed_X, y in val_dataloader:\n",
    "            raw_X, processed_X, y = raw_X.to(device, non_blocking=True), processed_X.to(device, non_blocking=True), y.to(device)\n",
    "\n",
    "            val_y_pred = model(raw_X, processed_X)\n",
    "\n",
    "            val_loss += loss_func(val_y_pred, y).item()\n",
    "\n",
    "            # accuracy\n",
    "            correct += (val_y_pred.argmax(dim=1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "        val_loss /= len(val_dataloader)\n",
    "        val_acc = 100.0 * correct / total\n",
    "\n",
    "    print(f\"Val loss: {val_loss:.5f} | Val acc: {val_acc:.2f}%\\n\")\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "def train_and_evaluate(model, epochs, patience, min_delta, train_dataloader, val_dataloader, loss_func, optimizer,\n",
    "                       device):\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "    val_loss_list = []\n",
    "    val_acc_list = []\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model, train_dataloader, loss_func, optimizer, device)\n",
    "        val_loss, val_acc = val_step(model, val_dataloader, loss_func, device)\n",
    "\n",
    "        train_loss_list.append(train_loss)\n",
    "        train_acc_list.append(train_acc)\n",
    "        val_loss_list.append(val_loss)\n",
    "        val_acc_list.append(val_acc)\n",
    "\n",
    "        if val_loss < best_val_loss - min_delta:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "            best_model_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        else:\n",
    "            counter += 1\n",
    "            print(f\"EarlyStopping counter: {counter}/{patience}\")\n",
    "\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return train_loss_list, train_acc_list, val_loss_list, val_acc_list\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Test\n",
    "# =====================================================\n",
    "\n",
    "def test_step(model, test_data_loader, loss_func, device):\n",
    "    test_loss, test_acc = 0, 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_test_list = []\n",
    "    y_pred_list = []\n",
    "    y_pred_prob_list = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for raw_X, processed_X, y in test_data_loader:\n",
    "            raw_X, processed_X, y = raw_X.to(device, non_blocking=True), processed_X.to(device, non_blocking=True), y.to(device)\n",
    "\n",
    "            test_y_pred = model(raw_X, processed_X)\n",
    "            y_pred_prob_list.append(torch.softmax(test_y_pred, dim=1))\n",
    "\n",
    "            test_loss += loss_func(test_y_pred, y).item()\n",
    "\n",
    "            y_test_list.append(y.cpu())\n",
    "            y_pred_list.append(test_y_pred.argmax(dim=1).cpu())\n",
    "\n",
    "            correct += (test_y_pred.argmax(dim=1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "        test_loss /= len(test_data_loader)\n",
    "        test_acc = 100.0 * correct / total\n",
    "\n",
    "    print(f\"Test loss: {test_loss:.5f} | Test acc: {test_acc:.2f}%\\n\")\n",
    "    return y_test_list, y_pred_list, y_pred_prob_list, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "40204f63300d4f37a5bd21a2caf7ca86",
      "ab230486d1a84be89a0fd5ea591348ae",
      "340b2952eebe4fd190d9699660374b54",
      "2e641ebd41064fb8bb46708ed208d4e5",
      "188ba7e350b0427584eebede1f041a48",
      "f32afa0940dc4365aba4c725b5ebc2f9",
      "37ad6319210f4791a53897bdaa4672aa",
      "3479227a449f4147a2a3aeb7ebb5ff1c",
      "3d4ebc3e96684cfca593a69ff87d00f1",
      "becfea95b5f942129e7423ab50f1bec0",
      "15389a7260c84e44ac9d706d62b7f271"
     ]
    },
    "executionInfo": {
     "elapsed": 103012,
     "status": "ok",
     "timestamp": 1770438998008,
     "user": {
      "displayName": "Inzaman Sheshan",
      "userId": "03733916943512366575"
     },
     "user_tz": -330
    },
    "id": "1ZnX8JmMGB5y",
    "outputId": "adcb007d-4b0c-4910-f3d6-6c4281a590c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 3181\n",
      "Number of validation samples: 908\n",
      "Number of testing samples: 456\n",
      "Length of TrainDataloader: 100 batches of 32\n",
      "Length of ValDataloader: 29 batches of 32\n",
      "Length of TestDataloader: 15 batches of 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40204f63300d4f37a5bd21a2caf7ca86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.64465 | Train acc: 70.26%\n",
      "\n",
      "Val loss: 0.53220 | Val acc: 74.23%\n",
      "\n",
      "Train loss: 0.41148 | Train acc: 82.11%\n",
      "\n",
      "Val loss: 0.45327 | Val acc: 76.10%\n",
      "\n",
      "Train loss: 0.31895 | Train acc: 87.14%\n",
      "\n",
      "Val loss: 0.29948 | Val acc: 89.32%\n",
      "\n",
      "Train loss: 0.27554 | Train acc: 88.93%\n",
      "\n",
      "Val loss: 0.27408 | Val acc: 89.65%\n",
      "\n",
      "Train loss: 0.23200 | Train acc: 91.01%\n",
      "\n",
      "Val loss: 0.26022 | Val acc: 91.74%\n",
      "\n",
      "Train loss: 0.20517 | Train acc: 92.11%\n",
      "\n",
      "Val loss: 0.23544 | Val acc: 91.85%\n",
      "\n",
      "Train loss: 0.18246 | Train acc: 93.40%\n",
      "\n",
      "Val loss: 0.21131 | Val acc: 93.39%\n",
      "\n",
      "Train loss: 0.16533 | Train acc: 94.22%\n",
      "\n",
      "Val loss: 0.19838 | Val acc: 94.38%\n",
      "\n",
      "Train loss: 0.13935 | Train acc: 95.06%\n",
      "\n",
      "Val loss: 0.18412 | Val acc: 95.59%\n",
      "\n",
      "Train loss: 0.12977 | Train acc: 94.94%\n",
      "\n",
      "Val loss: 0.18123 | Val acc: 95.37%\n",
      "\n",
      "Train loss: 0.11002 | Train acc: 96.23%\n",
      "\n",
      "Val loss: 0.17473 | Val acc: 95.26%\n",
      "\n",
      "Train loss: 0.10014 | Train acc: 96.13%\n",
      "\n",
      "Val loss: 0.17970 | Val acc: 95.37%\n",
      "\n",
      "EarlyStopping counter: 1/7\n",
      "Train loss: 0.09501 | Train acc: 96.48%\n",
      "\n",
      "Val loss: 0.16782 | Val acc: 96.48%\n",
      "\n",
      "Train loss: 0.08423 | Train acc: 97.23%\n",
      "\n",
      "Val loss: 0.17506 | Val acc: 96.04%\n",
      "\n",
      "EarlyStopping counter: 1/7\n",
      "Train loss: 0.06968 | Train acc: 97.74%\n",
      "\n",
      "Val loss: 0.16924 | Val acc: 95.37%\n",
      "\n",
      "EarlyStopping counter: 2/7\n",
      "Train loss: 0.05867 | Train acc: 97.89%\n",
      "\n",
      "Val loss: 0.18001 | Val acc: 96.04%\n",
      "\n",
      "EarlyStopping counter: 3/7\n",
      "Train loss: 0.06035 | Train acc: 97.83%\n",
      "\n",
      "Val loss: 0.18992 | Val acc: 96.48%\n",
      "\n",
      "EarlyStopping counter: 4/7\n",
      "Train loss: 0.04973 | Train acc: 98.24%\n",
      "\n",
      "Val loss: 0.16059 | Val acc: 96.37%\n",
      "\n",
      "Train loss: 0.03894 | Train acc: 98.87%\n",
      "\n",
      "Val loss: 0.19606 | Val acc: 96.81%\n",
      "\n",
      "EarlyStopping counter: 1/7\n",
      "Train loss: 0.03907 | Train acc: 98.68%\n",
      "\n",
      "Val loss: 0.17688 | Val acc: 97.03%\n",
      "\n",
      "EarlyStopping counter: 2/7\n",
      "Train loss: 0.03520 | Train acc: 98.59%\n",
      "\n",
      "Val loss: 0.18699 | Val acc: 97.03%\n",
      "\n",
      "EarlyStopping counter: 3/7\n",
      "Train loss: 0.03099 | Train acc: 98.96%\n",
      "\n",
      "Val loss: 0.35832 | Val acc: 91.30%\n",
      "\n",
      "EarlyStopping counter: 4/7\n",
      "Train loss: 0.03095 | Train acc: 98.90%\n",
      "\n",
      "Val loss: 0.20534 | Val acc: 96.70%\n",
      "\n",
      "EarlyStopping counter: 5/7\n",
      "Train loss: 0.03070 | Train acc: 99.03%\n",
      "\n",
      "Val loss: 0.19290 | Val acc: 96.70%\n",
      "\n",
      "EarlyStopping counter: 6/7\n",
      "Train loss: 0.02518 | Train acc: 99.18%\n",
      "\n",
      "Val loss: 0.18402 | Val acc: 97.03%\n",
      "\n",
      "EarlyStopping counter: 7/7\n",
      "Early stopping triggered\n",
      "Test loss: 0.10451 | Test acc: 96.93%\n",
      "\n",
      "Test: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "Predicted: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "Predicted Probability: [[9.51045871e-01 4.89535555e-02 5.65828032e-07]\n",
      " [9.93155181e-01 6.80190278e-03 4.28806161e-05]\n",
      " [5.05824566e-01 4.94175375e-01 5.97505867e-09]\n",
      " ...\n",
      " [1.13028655e-07 2.64740502e-05 9.99973416e-01]\n",
      " [1.85887271e-19 7.43997197e-10 1.00000000e+00]\n",
      " [1.48312657e-17 1.35280914e-07 9.99999881e-01]]\n",
      "Metrics saved to experiments/2026_02_07_exp_026_final_custom_cnn_configuration_dropout_0.5_with_early_stopping/metrics.json\n",
      "Train/Val metrics saved to train_val_metrics.csv\n",
      "Test predictions saved to test_predictions.csv\n",
      "Environment Info saved to experiments/2026_02_07_exp_026_final_custom_cnn_configuration_dropout_0.5_with_early_stopping/env_info.json\n"
     ]
    }
   ],
   "source": [
    "experiment_path = Path(\"experiments/2026_02_07_exp_026_final_custom_cnn_configuration_dropout_0.5_with_early_stopping\")\n",
    "\n",
    "config = load_config(experiment_path / \"config.yaml\")\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "\n",
    "random.seed(config[\"seed\"])\n",
    "np.random.seed(config[\"seed\"])\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "torch.cuda.manual_seed(config[\"seed\"])\n",
    "torch.cuda.manual_seed_all(config[\"seed\"])\n",
    "\n",
    "device = \"cuda\" if config[\"device\"] == \"cuda\" and torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = dataset(Path(\"data/processed/mri\"), config)\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = dataloader(config, train_dataset, val_dataset, test_dataset)\n",
    "\n",
    "classes = ['glioma', 'meningioma', 'pituitary']\n",
    "\n",
    "model, loss_func, optimizer = make_model(config, classes, device)\n",
    "\n",
    "train_loss_list, train_acc_list, val_loss_list, val_acc_list = train_and_evaluate(model,\n",
    "                                                                                    config[\"training\"][\"epochs\"],\n",
    "                                                                                    config[\"training\"][\"patience\"],\n",
    "                                                                                    config[\"training\"][\"min_delta\"],\n",
    "                                                                                    train_dataloader, val_dataloader,\n",
    "                                                                                    loss_func, optimizer, device)\n",
    "\n",
    "checkpoint = {\n",
    "    \"epoch\": config[\"training\"][\"epochs\"],\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"config\": config,\n",
    "    \"best_val_loss\": np.min(val_loss_list),\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, experiment_path / \"checkpoint.pth\")\n",
    "\n",
    "y_test_list, y_pred_list, y_pred_prob_list, test_accuracy = test_step(model, test_dataloader, loss_func, device)\n",
    "\n",
    "y_test_list = torch.cat(y_test_list).numpy()  # true labels\n",
    "y_pred_list = torch.cat(y_pred_list).numpy()  # predicted class\n",
    "y_pred_prob_list = torch.cat(y_pred_prob_list).cpu().numpy()  # predicted probabilities\n",
    "\n",
    "print(f\"Test: {y_test_list}\")\n",
    "print(f\"Predicted: {y_pred_list}\")\n",
    "print(f\"Predicted Probability: {y_pred_prob_list}\")\n",
    "\n",
    "# Convert metrics to JSON-friendly format\n",
    "metrics = {\n",
    "    \"train_loss\": [float(l) for l in train_loss_list],\n",
    "    \"train_accuracy\": [float(a) for a in train_acc_list],\n",
    "    \"val_loss\": [float(l) for l in val_loss_list],\n",
    "    \"val_accuracy\": [float(a) for a in val_acc_list],\n",
    "    \"test_accuracy\": test_accuracy,\n",
    "    \"classes\": classes,\n",
    "    \"epochs\": config[\"training\"][\"epochs\"],\n",
    "    \"num_train_samples\": len(train_dataset),\n",
    "    \"num_val_samples\": len(val_dataset),\n",
    "    \"num_test_samples\": len(test_dataset),\n",
    "    \"best_val_epoch\": int(np.argmin(val_loss_list)) + 1,\n",
    "    \"best_val_loss\": float(np.min(val_loss_list)),\n",
    "    \"best_val_accuracy\": float(val_acc_list[np.argmin(val_loss_list)])\n",
    "}\n",
    "\n",
    "# Save metrics\n",
    "metrics_path = experiment_path / \"metrics.json\"\n",
    "with open(metrics_path, \"w\") as f:\n",
    "    json.dump(metrics, f, indent=4)\n",
    "\n",
    "print(f\"Metrics saved to {metrics_path}\")\n",
    "\n",
    "# Create DataFrame\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"epoch\": list(range(len(train_loss_list))),\n",
    "    \"train_loss\": train_loss_list,\n",
    "    \"train_accuracy\": [a for a in train_acc_list],\n",
    "    \"val_loss\": val_loss_list,\n",
    "    \"val_accuracy\": [a for a in val_acc_list]\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "metrics_df.to_csv(experiment_path / \"train_val_metrics.csv\", index=False)\n",
    "print(\"Train/Val metrics saved to train_val_metrics.csv\")\n",
    "\n",
    "# Convert predicted probabilities to a DataFrame\n",
    "prob_df = pd.DataFrame(y_pred_prob_list, columns=[f\"prob_{cls}\" for cls in classes])\n",
    "\n",
    "# Create main DataFrame\n",
    "test_df = pd.DataFrame({\"y_true\": y_test_list, \"y_pred\": y_pred_list})\n",
    "\n",
    "# Concatenate probabilities\n",
    "test_df = pd.concat([test_df, prob_df], axis=1)\n",
    "\n",
    "# Save to CSV\n",
    "test_df.to_csv(experiment_path / \"test_predictions.csv\", index=False)\n",
    "print(\"Test predictions saved to test_predictions.csv\")\n",
    "\n",
    "env_info = {\n",
    "    \"python_version\": platform.python_version(),\n",
    "    \"pytorch_version\": torch.__version__,\n",
    "    \"cuda_version\": torch.version.cuda,\n",
    "    \"cudnn_version\": torch.backends.cudnn.version(),\n",
    "    \"gpu\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,\n",
    "    \"os\": platform.platform(),\n",
    "}\n",
    "\n",
    "env_path = experiment_path / \"env_info.json\"\n",
    "with open(env_path, \"w\") as f:\n",
    "    json.dump(env_info, f, indent=4)\n",
    "\n",
    "print(f\"Environment Info saved to {env_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1770438998032,
     "user": {
      "displayName": "Inzaman Sheshan",
      "userId": "03733916943512366575"
     },
     "user_tz": -330
    },
    "id": "C2FhPMjqfIVt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNT38VC+oRbnDAljQyAWNvN",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
